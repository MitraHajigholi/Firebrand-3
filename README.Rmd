---
title: "Applied Data Science"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
Packages we'll look at today:

- odbc / readxl / readr / dbplyr for data access
- tidyverse for data manipulation
- DataExplorer for helping us providing our EDA of the data
- modelr / rsamples for sampling strategy
- recipes for performing feature engineering
- glmnet / glmnetUtils / h2o / FFTrees for building models
- yardstick / broom for evaluation
- rmarkdown for documentation

## Working with databases
We need a database connection before we can do anything with our database
```{r}
library(DBI)  # talk with databases, a driver
library(odbc) # allows us to talk with DBI drivers

driver = "SQL server" # prgram that allows us to talk with the database
server = "fbmcsads.database.windows.net"
database = "WideWorldImporters-Standard"
uid = "adatumadmin"
pwd = "Pa55w.rdPa55w.rd"


con <- dbConnect(odbc(),
                 driver = driver,
                 server = server,
                 database = database,
                 uid = uid,
                 pwd = pwd)
```

Now that we have a DB connection, we can write SQL in a code chunk.
```{sql connection=con}
select top 5 * from flights

```


We can use dbplyr to construct dplyr commands that work on the DB.

```{r}
library(dbplyr) # translates into sql 
library(tidyverse) # group_by and filter and stuff in this package
flights_tbl <- tbl(con, "flights")  # talk with database con and flights table

flights_tbl %>% 
  filter(month<=6) %>%  
  group_by(origin) %>% 
  summarise(n = n(), # n is number of rows
            mean_dist = mean(distance)) %>% 
  show_query()

```

We can also work with tables that aren't in the default schema.
```{r}
purchaseorders_tbl <-tbl(con, in_schema("purchasing", "purchaseorders")) # selects purchaseorders in purchasing 

purchaseorders_tbl %>%  
  top_n(5)
```

We can use the 'Id()' function from DBI to work with schema more generically within a database. This means we aren't restricted to just SELECT statements.

```{r error = TRUE} 
# error = true, we will se the error code it generated
dbGetQuery(con,"CREATE SCHEMA DBIexample5")  # insert a number for your examlpe
dbWriteTable(con,"iris", iris, overwrite = TRUE)
#Read from newly written table
head(dbReadTable(con,"iris"))
#Read from a table in a schema
head(dbReadTable(con, Id(schema="20774A", table = "CustomerTransactions")))
#If a write methid is supportewd by the driver, this will work
dbWriteTable(con, Id(schema="DBIexampleMitra", table = "iris", iris, overwrite = TRUE))
```

Some of our code could fail in that section so we used 'error=TRUE' to be able to carry on even if some of the code error-ed. Great for optional code or things with bad connections.


## Exploratory

```{r eval = FALSE}
## eval = FALSE, should evaluate this chunk of code or not, when I nit it.
flights_tbl %>% 
  as_data_frame() %>% 
  DataExplorer::GenerateReport()

```


Questions arising from the basic report:

1. Why is there a day with double the number of flights?
2. We need to address the high correlation between time columns
3. Why is there negative correlation between 'flight' and 'distance'?
4. Do we need to do anything about missings or can we just remove the rows
5. look up why there is a peak in middle of the month?

Things to implement later in the workflow due to the EDA (explorer tree data analysis):

1. We need to address the high correlation between time columns
2. We need to group low freq airlines carries
3. Bi variate for analyzing two things in relation to each other

### Answering our questions

> Why is there a day with double the number of flights?

Are there duplicate rows?

```{r}
flights_tbl %>% 
  filter(day == 15) %>% 
  distinct()  %>%  
  summarise(n())  %>%  # count the data
  as_data_frame() ->  # force it to give it the content and not sql code
  distict_count  # create a uniqe list of using distict count
  
  # get all rows if dublicate or not
flights_tbl %>% 
  filter(day == 15) %>% 
  summarise(n())  %>%  
  as_data_frame() ->
  row_count 

# if the sructure and values are the same return true,
identical(row_count, distict_count) #one row per observed flight?

```

But are the number of rows unusual?

```{r}
library(ggplot2)
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n = n(), n_distinct(flight)) %>% 
  arrange(day)

## to plot the data instead : 
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n = n(), n_distinct(flight)) %>%
  as_data_frame() %>% 
  ggplot(aes(day,y = n)) + geom_col()  # does not do any binning of the data

```
Data is fine, the problem is in our visualization. Doing histogram, split continues numbers have to group them two in a day. the spike looks to be a problem but its not.

Looks like the jump in the histogram is an artifact of binning the data. s'oh!


### Bivariate analysis
```{r}

flights_tbl %>% 
  select_if(is.numeric) %>% 
  as_data_frame() %>%   # need to convert to a data frame from sql data
  gather(col,val,-dep_delay) %>% # dont pivot the dep_delay column, gather and pivot creates the variables col and val
  filter(col!= "arr_delay", dep_delay < 500) %>%  # filter out arr_delay to see better result
  ggplot(aes(x=val, y=dep_delay)) + 
  #geom_point() +  # this takes long time since its plotting row by row
  geom_bin2d() +
  facet_wrap(~col, scales = "free") + # takes different parts of our data to produce them as charts
  scale_fill_gradientn(colours= viridisLite::viridis(256, option = "D"))

```



### Sampling

Our options for sampling data with large class imbalance are:

- Down sampling takes as many majority rows and there are minority rows
    + No over fit from individual rows
    + Can drastically reduce training data size

- Up sampling or sampling repeats minority rows until they meet some defined class ratio
    + Risks over fitting
    + Doesn't reduce training data set
    
- Synthesizing data makes extra records that are like the minority class
    + Does not reduce training set
    + Avoids some of the over fit risk of up sampling
    + Can weaken predictions if minority data is very similar to majority
    + synthpop is a package used. 
 

We need to think about whether we need to k-fold cross-validation explicitly.

- Run the same model and assess robustness of the coefficients
- We have an algorithm that needs explicit cross validation because it does not do it internally
- When we are going to run lots of models with hyper-parameter tuning so the results are more consistent.

We use bootstrapping when we want to fit a single model and ensure the results are robust. 
This will often do many more iterations than k-fold cross validation, making it better in cases where there is relatively small amounts of data. 
(Bootstrapping is used to arrive at a single robust model)

Packages we can use for sampling include:

- modelr which facilitates boostrap and cross validation strategies 
- rsample allows us to bootstrap and perform a wide variety of cross validation tasks
- recipes allows us to up sample and down sample
- synthpop allows us to build synthesized samples

    
More notes:
- we have many underlays (= majority of the data), how can we predict when the flights are late, now it will always say that the flights are on time... what to do?
- Down sampling = if we have alot of data, take randome small sample of the not delayed majority class data and still have a huge amount of data, having more data of delayed. 
- upsampling = a model to predict 3 % that are delay, repeat those rows which are delays, known minority class rows, until we have 15 - 20% delays.
- synthesize data, is the third option

Hyperparameter tunings
- Use training data
- Split our data into training and test set. 4/5 split, 


Need to test different models to select the one that gives the best result. 


### Install 
# from github, it is CRAN but we have an earlier version
# run in console >
#install.packages("devtools")
#devtools::install_github("topepo/recipes")


### Practical
First we need to split our data into test and train.

```{r}
flights_tbl %>% 
  as_data_frame() ->
  flights

flights %>% 
  mutate(was_delayed = ifelse(arr_delay>5, "Delayed", "Not Delayed"), week = ifelse(day %/% 7 >3, 3, day %/% 7))  -> # flight delayed with 5 min & %/% makes absolute division (decimal < 1 becomes 0 when doing division and 1 when one 7 fits to the value, 2 for 2 7:ns osv..), we create weeks, 
  flights

flights %>% 
  modelr::resample_partition(c(train=0.7, test =0.3)) ->
  splits

splits %>% 
  pluck("train") %>% 
  as_data_frame() ->
  train_raw

splits %>% 
  pluck("test") %>% 
  as_data_frame() ->
  test_raw

  
```

Druring the investigation, we will look at the impact of upsampling. We will see it in action in a bit. First prepping our basic features!

```{r}
library(recipes)

basic_fe <- recipe(train_raw, was_delayed ~ .)  # ~ . = by everything to predict was_delayed data # feature engineering

basic_fe %>% 
  step_rm(ends_with("time"), ends_with("delay"), 
          year, day, minute, time_hour, tailnum, flight) %>% 
#  step_corr(all_predictors()) %>%  # remove highly corr variables
  step_zv(all_predictors()) %>% # remove parameters with near zero variance
  step_nzv(all_predictors()) %>%  # drop param. if freq is less than 5 % that has differences
  step_naomit(all_predictors()) %>% #remove recors that are NA, because they are only 3%
  step_naomit(all_outcomes()) %>%
  step_other(all_nominal(), threshold = 0.03) ->  # if they have values with low incident rate to a low category, other category. nominal = categorical variable
 # step_discretize(month, day) ->  #convert from numbers to category variables
  colscleaned_fe
  
colscleaned_fe

colscleaned_fe <- prep(colscleaned_fe, verbose = TRUE)
colscleaned_fe

# do what we prepped to do
train_prep1 <- bake(colscleaned_fe, train_raw)

```

Now we need to process our numeric variables.



```{r}

colscleaned_fe %>% 
  step_num2factor(month, week, hour) %>% # factors the numbers into classifications
  step_rm(tailnum) %>% #hack! now removed!
  step_log(distance) ->
  numscleaned_fe

numscleaned_fe <- prep(numscleaned_fe, verbose = TRUE)
numscleaned_fe

train_prep1 <- bake(numscleaned_fe, train_raw)


```



W00t its upsampling time!

```{r}
# mean(train_prep1$was_delayed,na.rm =TRUE)  # just to check the mean
numscleaned_fe %>% 
  step_upsample(all_outcomes(), ratio = 1.) %>%  # increase to 50 % ratio between 
  prep(retain = TRUE) %>% 
  juice() %>% 
  #hack because juice is not reducing the column set
  bake(numscleaned_fe, .) ->
  train_prep2

```


## Building models
Decide which types of models you want to consider -- perhaps using Microsoft's lovely [cheat sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet). Then determine if any need any special processing to the data beyond what you have done so far.


### A basic logistic regression model
```{r}
glm_unbal <- glm(was_delayed~ . -1,"binomial", data = train_prep1) # "was_delayed"" = input variable AND "~ ."" = output variable
glm_bal <- glm(was_delayed~ . -1, "binomial", data = train_prep2) # general linearized model, general regression model

```
Then we can see how these models are constructed and how they perform

```{r}
library(broom)
glance(glm_unbal)  # model obj linear model, coeff values, how we change prop of an outcome...
# BIC = how much information we are collecting
# logLik = measure of what predicted and what happened ?? come back to this later


```


Get the coefficients of the model
```{r}
tidy(glm_unbal)
```

Takes original data and suppliment with predicte data and predicted error and associate with those...????
```{r}
head(augment(glm_unbal))
```


Plot predictive's vs actuals


```{r}
glm_unbal %>% 
  augment() %>% 
  ggplot(aes(x=.fitted, group=was_delayed, fill= was_delayed)) + 
  geom_density(alpha=0.5) + 
  geom_vline(aes(xintercept=0))

# if its x>0, then its more than 50 % chance that is not delayed. 
# if x<0 then its more than 50 % chance that its delayed.


```

#### Prep and predict on test data
```{r}
test_raw %>% 
  bake(numscleaned_fe, .) %>% 
  modelr:: add_predictions(glm_unbal,var = "glm_unbal") ->
  test_scored

```

```{r}
test_scored %>% 
  ggplot(aes(x=glm_unbal, group=was_delayed, fill= was_delayed)) + 
  geom_density(alpha=0.5) + 
  geom_vline(aes(xintercept=0))

# if its x>0, then its more than 50 % chance that is not delayed. 
# if x<0 then its more than 50 % chance that its delayed.


```


But how many did we get right etc?


```{r}
library(yardstick)
test_scored %>% 
  mutate(glm_unbal_class = as.factor(ifelse(glm_unbal<0, "Delayed", "Not Delayed"))) %>% 
  conf_mat(was_delayed, glm_unbal_class)



```


```{r}
test_scored %>% 
  mutate(glm_unbal_class = as.factor(
    ifelse(glm_unbal<0, "Delayed", "Not Delayed"))) %>% 
  accuracy(was_delayed, glm_unbal_class)

```

