---
title: "Applied Data Science"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
Packages we'll look at today:

- odbc / readxl / readr / dbplyr for data access
- tidyverse for data manipulation
- DataExplorer for helping us providing our EDA of the data
- modelr / rsamples for sampling strategy
- recipes for performing feature engineering
- glmnet / glmnetUtils / h2o / FFTrees for building models
- yardstick / broom for evaluation
- rmarkdown for documentation

## Working with databases
We need a database connection before we can do anything with our database
```{r}
library(DBI)  # talk with databases, a driver
library(odbc) # allows us to talk with DBI drivers

driver = "SQL server" # prgram that allows us to talk with the database
server = "fbmcsads.database.windows.net"
database = "WideWorldImporters-Standard"
uid = "adatumadmin"
pwd = "Pa55w.rdPa55w.rd"


con <- dbConnect(odbc(),
                 driver = driver,
                 server = server,
                 database = database,
                 uid = uid,
                 pwd = pwd)
```

Now that we have a DB connection, we can write SQL in a code chunk.
```{sql connection=con}
select top 5 * from flights

```


We can use dbplyr to construct dplyr commands that work on the DB.

```{r}
library(dbplyr) # translates into sql 
library(tidyverse) # group_by and filter and stuff in this package
flights_tbl <- tbl(con, "flights")  # talk with database con and flights table

flights_tbl %>% 
  filter(month<=6) %>%  
  group_by(origin) %>% 
  summarise(n = n(), # n is number of rows
            mean_dist = mean(distance)) %>% 
  show_query()

```

We can also work with tables that aren't in the default schema.
```{r}
purchaseorders_tbl <-tbl(con, in_schema("purchasing", "purchaseorders")) # selects purchaseorders in purchasing 

purchaseorders_tbl %>%  
  top_n(5)
```

We can use the 'Id()' function from DBI to work with schema more generically within a database. This means we aren't restricted to just SELECT statements.

```{r error = TRUE} 
# error = true, we will se the error code it generated
dbGetQuery(con,"CREATE SCHEMA DBIexample5")  # insert a number for your examlpe
dbWriteTable(con,"iris", iris, overwrite = TRUE)
#Read from newly written table
head(dbReadTable(con,"iris"))
#Read from a table in a schema
head(dbReadTable(con, Id(schema="20774A", table = "CustomerTransactions")))
#If a write methid is supportewd by the driver, this will work
dbWriteTable(con, Id(schema="DBIexampleMitra", table = "iris", iris, overwrite = TRUE))
```

Some of our code could fail in that section so we used 'error=TRUE' to be able to carry on even if some of the code errored. Great for optional code or things with bad connections.


## Exploratory

```{r eval = FALSE}
## eval = FALSE, should evaluate this chunk of code or not, when I nit it.
flights_tbl %>% 
  as_data_frame() %>% 
  DataExplorer::GenerateReport()

```


Questions arising frmo the basic report:

1. Why is there a day with double the number of flights?
2. We need to address the high correlation between time columns
3. Why is there negative correlation between 'flight' and 'distance'?
4. Do we need to do anything about missings or can we just remove the rows
5. look up why there is a peak in middle of the month?

Things to imlpement later in the workflow due to the EDA (explorer tree data analysis):

1. We need to address the high correlation beteen time columns
2. We need to group low freq airlines carries
3. Bivariate for anlyzing two things in realtion to each other

### Answering our questions

> Why is there a day with double the number of flights?

Are there dublicate rows?

```{r}
flights_tbl %>% 
  filter(day == 15) %>% 
  distinct()  %>%  
  summarise(n())  %>%  # count the data
  as_data_frame() ->  # force it to give it the content and not sql code
  distict_count  # create a uniqe list of using distict count
  
  # get all rows if dublicate or not
flights_tbl %>% 
  filter(day == 15) %>% 
  summarise(n())  %>%  
  as_data_frame() ->
  row_count 

# if the sructure and values are the same return true,
identical(row_count, distict_count) #one row per observed flight?

```

But are the number of rows unusual?

```{r}
library(ggplot2)
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n = n(), n_distinct(flight)) %>% 
  arrange(day)

## to plot the data instead : 
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n = n(), n_distinct(flight)) %>%
  as_data_frame() %>% 
  ggplot(aes(day,y = n)) + geom_col()  # does not do any binning of the data

```
Data is fine, the problem is in our visualization. Doing histogram, split continus numbers have to group them two in a day. the spike looks to be a problem but its not.

Looks like the jump in the histogram is an artifact of binning the data. s'oh!


### Bivariate analysis
```{r}

flights_tbl %>% 
  select_if(is.numeric) %>% 
  as_data_frame() %>%   # need to convert to a data frame from sql data
  gather(col,val,-dep_delay) %>% # dont pivot the dep_delay column, gather and pivot creates the variables col and val
  filter(col!= "arr_delay", dep_delay < 500) %>%  # filter out arr_delay to see better result
  ggplot(aes(x=val, y=dep_delay)) + 
  #geom_point() +  # this takes long time since its plotting row by row
  geom_bin2d() +
  facet_wrap(~col, scales = "free") + # takes different parts of our data to produce them as charts
  scale_fill_gradientn(colours= viridisLite::viridis(256, option = "D"))

```



### Sampling

Our options for sampling data with large class imbalance are:

- Downsampling takes as many majority rows and there are minority rows
    + No overfit from individual rows
    + Can drastically reduce training data size

- Upsampling or sampling repeats minority rows until they meet some defined class ratio
    + Risks overfitting
    + Doesn't reduce training data set
    
- Synthesising data makes extra records that are like the minority class
    + Does not reduce training set
    + Avoids some of the overfit risk of upsampling
    + Can weaken predictions if minority data is very similar to majority
    + synthpop is a package used. 
 

We need to think about wheter we need to k-fold cross-validation explicitly.

- Run the same model and assess robustness of the coefficients
- We have an algorithm that needs explicit cross validation because it does not do it internally
- When we are going to run lots of models with hyper-parameter tuning so the results are more consistent.

We use bootstrapping when we want to fit a single model and ensure the results are robust. 
This will often do many more iterations than k-fold cross validation, making it better in cases where there is reletively small amounts of data. 
(Bootstrapping is used to arrive at a single robust model)

Packages we can use for sampling include:

- modelr which facilitates boostrap and crossvalidation strategies 
- rsample allows us to bootstrap and perform a wide variety of crossvalidation tasks
- recipes allows us to upsample and downsample
- synthpop allows us to build synthesised samples

    
More notes:
- we have many undelays (= majority of the data), how can we predict when the flights are late, now it will always say that the lflights are on time... what to do?
- Downsampling = if we have alot of data, take randome small sample of the not delayed majority class data and still have a huge amount of data, having more data of delayed. 
- upsampling = a model to predict 3 % that are delay, repeat those rows which are delays, known minority class rows, until we have 15 - 20% delays.
- synthesize data, is the third option

Hyperparameter tunings
- Use training data
- Split our data into training and test set. 4/5 split, 


Need to test different models to select the one that gives the best result. 


### Install 
# from github, it is CRAN but we have an earlier version
# run in console >
#install.packages("devtools")
#devtools::install_github("topepo/recipes")


### Practical
First we need to split our data into test and train.

```{r}
flights_tbl %>% 
  as_data_frame() ->
  flights

flights %>% 
  mutate(was_delayed = ifelse(arr_delay>5, "Delayed", "Not Delayed"), week = ifelse(day %/% 7 >3, 3, day %/% 7))  -> # flight delayed with 5 min & %/% makes absolute division (decimal < 1 becomes 0 when doing division and 1 when one 7 fits to the value, 2 for 2 7:ns osv..), we create weeks, 
  flights

flights %>% 
  modelr::resample_partition(c(train=0.7, test =0.3)) ->
  splits

splits %>% 
  pluck("train") %>% 
  as_data_frame() ->
  train_raw

splits %>% 
  pluck("test") %>% 
  as_data_frame() ->
  test_raw

  
```

Druring the investigation, we will look at the impact of upsampling. We will see it in action in a bit. First prepping our basic features!

```{r}
library(recipes)

basic_fe <- recipe(train_raw, was_delayed ~ .)  # ~ . = by everything to predict was_delayed data # feature engineering

basic_fe %>% 
  step_rm(ends_with("time"), ends_with("delay"), 
          year, day, minute, time_hour, tailnum, flight) %>% 
#  step_corr(all_predictors()) %>%  # remove highly corr variables
  step_zv(all_predictors()) %>% # remove parameters with near zero variance
  step_nzv(all_predictors()) %>%  # drop param. if freq is less than 5 % that has differences
  step_naomit(all_predictors()) %>% #remove recors that are NA, because they are only 3%
  step_other(all_nominal(), threshold = 0.03) ->  # if they have values with low incident rate to a low category, other category. nominal = categorical variable
 # step_discretize(month, day) ->  #convert from numbers to category variables
  colscleaned_fe
  
colscleaned_fe

colscleaned_fe <- prep(colscleaned_fe, verbose = TRUE)
colscleaned_fe

# do what we prepped to do
train_prep1 <- bake(colscleaned_fe, train_raw)

```

Now we need to process our numeric variables.



```{r}

colscleaned_fe %>% 
  step_num2factor(month, week, hour) %>% # factors the numbers into classifications
  step_rm(tailnum) %>% #hack! now removed!
  step_log(distance) ->
  numscleaned_fe

numscleaned_fe <- prep(numscleaned_fe, verbose = TRUE)
numscleaned_fe

train_prep1 <- bake(numscleaned_fe, train_raw)


```



W00t its upsampling time!

```{r}
# mean(train_prep1$was_delayed,na.rm =TRUE)  # just to check the mean
numscleaned_fe %>% 
  step_upsample(all_outcomes(), ratio = 1.) %>%  # increase to 50 % ratio between 
  prep(retain = TRUE) %>% 
  juice() %>% 
  #hack because juice is not reducing the column set
  bake(numscleaned_fe, .) ->
  train_prep2

```







